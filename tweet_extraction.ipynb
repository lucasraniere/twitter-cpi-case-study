{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc91b0de-02c2-4fad-b000-4f9af65aedeb",
   "metadata": {},
   "source": [
    "# __Tweets Extraction: Brazilian Covid-19 CPI (Parliamentary Commission of Inquiry)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d64bf42-d54f-483f-ad30-9ab586b3253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.variables import hashtag_media, week_list\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import os, itertools, datetime, json\n",
    "from modules.week import Week\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f4d1cc-8bc0-42f1-90b6-ef19597a2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(hashtag_series):\n",
    "    hashtag_list = []\n",
    "    for hashtag_group in hashtag_series:\n",
    "        if hashtag_group:\n",
    "            for hashtag in hashtag_group:\n",
    "                if hashtag.lower() not in hashtag_media:\n",
    "                    hashtag_list.append('#'+hashtag.lower())\n",
    "    return hashtag_list\n",
    "\n",
    "def get_unique_hashtags(hashtag_list):\n",
    "    hashtag_series = pd.Series(hashtag_list)\n",
    "    return hashtag_series.unique()\n",
    "\n",
    "def get_hashtag_count(hashtag_list):\n",
    "    return get_unique_hashtags(hashtag_list).size\n",
    "\n",
    "def get_top10(hashtag_list):\n",
    "    hashtag_series = pd.Series(hashtag_list)\n",
    "    return hashtag_series.value_counts().index[:10].tolist()\n",
    "\n",
    "def day_tweet_extract(day):\n",
    "    until = (datetime.datetime.strptime(day, '%Y-%m-%d') + datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    query = f'cpi AND covid OR pandemia lang:pt since:{day} until:{until}'\n",
    "    print('- Cpi covid|pandemia query extraction:')\n",
    "    start_time = datetime.datetime.now()\n",
    "    tweets_q = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(query).get_items(), None))\n",
    "    runtime = datetime.datetime.now() - start_time\n",
    "    print(f'-- Runtime: {runtime}'\n",
    "         f'\\n-- Tweets amount: {tweets_q.shape[0]}')\n",
    "    hashtag_list_q = get_hashtags(tweets_q['hashtags'])\n",
    "    hashtag_count_q = get_hashtag_count(hashtag_list_q)\n",
    "    unique_hashtags_q = get_unique_hashtags(hashtag_list_q)\n",
    "    hashtag_top10_q = get_top10(hashtag_list_q)\n",
    "    print(f'-- Unique hashtags amount: {hashtag_count_q}'\n",
    "         f'\\n-- Top 10 Hashtags: {hashtag_top10_q}'\n",
    "         '\\n- Hashtag query extraction:')\n",
    "    hashtag_query = ' OR '.join(hashtag_top10_q)\n",
    "    query = f'({hashtag_query}) lang:pt since:{day} until:{until}'\n",
    "    start_time = datetime.datetime.now()\n",
    "    tweets_h = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(query).get_items(), None))\n",
    "    runtime = datetime.datetime.now() - start_time\n",
    "    print(f'-- Runtime: {runtime}'\n",
    "         f'\\n-- Tweets amount: {tweets_h.shape[0]}')\n",
    "    hashtag_list_h = get_hashtags(tweets_h['hashtags'])\n",
    "    hashtag_count_h = get_hashtag_count(hashtag_list_h)\n",
    "    unique_hashtags_h = get_unique_hashtags(hashtag_list_h)\n",
    "    hashtag_top10_h = get_top10(hashtag_list_h)\n",
    "    print(f'-- Unique hashtags amount: {hashtag_count_h}'\n",
    "         f'\\n-- Top 10 Hashtags: {hashtag_top10_h}\\n')\n",
    "    return {\n",
    "        'query_ext': {\n",
    "            'hashtag_count': hashtag_count_q,\n",
    "            'hashtag_top_10': hashtag_top10_q,\n",
    "            'unique_hashtags': unique_hashtags_q,\n",
    "            'tweets': tweets_q\n",
    "        },\n",
    "        'hashtag_ext': {\n",
    "            'hashtag_count': hashtag_count_h,\n",
    "            'hashtag_top_10': hashtag_top10_h,\n",
    "            'unique_hashtags': unique_hashtags_h,\n",
    "            'tweets': tweets_h\n",
    "        }\n",
    "    }\n",
    "\n",
    "def week_tweet_extract(week):\n",
    "    week_dir = f'data/tweets/week_{week.number}'\n",
    "    if not os.path.exists(week_dir):\n",
    "        os.mkdir(week_dir)\n",
    "    print(f'====== EXTRACTING FROM WEEK {week.number}: SINCE {week.start} UNTIL {week.end} ======'\n",
    "         f'\\n\\nData is being stored in the following directory: {week_dir}\\n')\n",
    "    week_info = week.info\n",
    "    week_hashtags = {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    }\n",
    "    week_tweets_amount = {\n",
    "        'query_ext': 0,\n",
    "        'hashtag_ext': 0\n",
    "    }\n",
    "    for idx, day in enumerate(week.days):\n",
    "        _day = f'day_{idx+1}'\n",
    "        deponents = week_info['days_info'][_day]['deponents']\n",
    "        print(f'Extracting tweets from Day: {idx+1}: {day}'\n",
    "             f'\\n- Deponents of the day: {deponents}')\n",
    "        day_ext = day_tweet_extract(day)\n",
    "        week_info['days_info'][_day]['tweets_amount'] = {\n",
    "            'query_ext': day_ext['query_ext']['tweets'].shape[0],\n",
    "            'hashtag_ext': day_ext['hashtag_ext']['tweets'].shape[0]\n",
    "        }\n",
    "        week_info['days_info'][_day]['hashtags_amount'] = {\n",
    "            'query_ext': day_ext['query_ext']['hashtag_count'],\n",
    "            'hashtag_ext': day_ext['hashtag_ext']['hashtag_count']\n",
    "        }\n",
    "        week_info['days_info'][_day]['top_10_hashtags'] = {\n",
    "            'query_ext': day_ext['query_ext']['hashtag_top_10'],\n",
    "            'hashtag_ext': day_ext['hashtag_ext']['hashtag_top_10']\n",
    "        }\n",
    "        week_tweets_amount = {\n",
    "            'query_ext': week_tweets_amount['query_ext'] + day_ext['query_ext']['tweets'].shape[0],\n",
    "            'hashtag_ext': week_tweets_amount['hashtag_ext'] + day_ext['hashtag_ext']['tweets'].shape[0]\n",
    "        }\n",
    "        for hashtag in day_ext['query_ext']['unique_hashtags']:\n",
    "            week_hashtags['query_ext'].append(hashtag)\n",
    "        for hashtag in day_ext['hashtag_ext']['unique_hashtags']:\n",
    "            week_hashtags['hashtag_ext'].append(hashtag)\n",
    "        day_ext['query_ext']['tweets'].astype(str).to_parquet(f'{week_dir}/{_day}_{day}_query_ext.parquet')\n",
    "        day_ext['hashtag_ext']['tweets'].astype(str).to_parquet(f'{week_dir}/{_day}_{day}_hashtags_ext.parquet')\n",
    "    week_info['tweets_amount'] = week_tweets_amount\n",
    "    week_info['hashtags_amount'] = {\n",
    "        'query_ext': get_hashtag_count(week_hashtags['query_ext']),\n",
    "        'hashtag_ext': get_hashtag_count(week_hashtags['hashtag_ext'])\n",
    "    }\n",
    "    week_info['top_10_hashtags'] = {\n",
    "        'query_ext': get_top10(week_hashtags['query_ext']),\n",
    "        'hashtag_ext': get_top10(week_hashtags['hashtag_ext'])\n",
    "    }\n",
    "    print('Generating week info json file...\\n')\n",
    "    with open(week_dir + f'/week_{week.number}_info.json', 'w') as json_file:\n",
    "        json.dump(week_info, json_file, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680245cf-af87-4562-90fa-59259887eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deponents = {\n",
    "#     #'day_3': [],\n",
    "#     #'day_4': [],\n",
    "#     #'day_5': []\n",
    "# }\n",
    "\n",
    "# week = Week(week_number=1,\n",
    "#            week_start='2021-04-25',\n",
    "#            deponents=deponents)\n",
    "\n",
    "# week_tweet_extract(week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0e8409-d34a-4863-9feb-743d833bef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== EXTRACTING FROM WEEK pr_04: SINCE 2021-10-10 UNTIL 2021-10-16 ======\n",
      "\n",
      "Data is being stored in the following directory: data/tweets/week_pr_04\n",
      "\n",
      "Extracting tweets from Day: 1: 2021-10-10\n",
      "- Deponents of the day: []\n",
      "- Cpi covid|pandemia query extraction:\n",
      "-- Runtime: 0:00:23.391077\n",
      "-- Tweets amount: 485\n",
      "-- Unique hashtags amount: 44\n",
      "-- Top 10 Hashtags: ['#cpidacovid', '#cpi', '#covid', '#forabolsonaro', '#ls', '#polícia', '#cpidocirco', '#puggina', '#senado', '#letinhodeminas']\n",
      "- Hashtag query extraction:\n",
      "-- Runtime: 0:01:24.225824\n",
      "-- Tweets amount: 2079\n",
      "-- Unique hashtags amount: 789\n",
      "-- Top 10 Hashtags: ['#forabolsonaro', '#cpidocirco', '#forabolsonarogenocida', '#cpidacovid', '#covid', '#forabolsonaroesuaquadrilha', '#impeachmentbolsonarourgente', '#lulapresidente2022', '#bolsonarogenocida', '#fechadoscombolsonaro']\n",
      "\n",
      "Extracting tweets from Day: 2: 2021-10-11\n",
      "- Deponents of the day: []\n",
      "- Cpi covid|pandemia query extraction:\n",
      "-- Runtime: 0:00:31.574317\n",
      "-- Tweets amount: 786\n",
      "-- Unique hashtags amount: 90\n",
      "-- Top 10 Hashtags: ['#cpidacovid', '#cpi', '#tácaroculpadobolsonaro', '#liradesistepec32', '#2outforabolsonaro', '#lulalivrebrasillivre', '#bolsonaro', '#brasil', '#pandemia', '#cpidapandemia']\n",
      "- Hashtag query extraction:\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "Unable to find guest token",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6312/3210926605.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[0mweek_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweek\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'week_start'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 deponents=week['deponents'])\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mweek_tweet_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_week\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6312/2817861197.py\u001b[0m in \u001b[0;36mweek_tweet_extract\u001b[1;34m(week)\u001b[0m\n\u001b[0;32m     83\u001b[0m         print(f'Extracting tweets from Day: {idx+1}: {day}'\n\u001b[0;32m     84\u001b[0m              f'\\n- Deponents of the day: {deponents}')\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mday_ext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mday_tweet_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         week_info['days_info'][_day]['tweets_amount'] = {\n\u001b[0;32m     87\u001b[0m             \u001b[1;34m'query_ext'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mday_ext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query_ext'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6312/2817861197.py\u001b[0m in \u001b[0;36mday_tweet_extract\u001b[1;34m(day)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'({hashtag_query}) lang:pt since:{day} until:{until}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mtweets_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msntwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTwitterSearchScraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     print(f'-- Runtime: {runtime}'\n",
      "\u001b[1;32m~\\anaconda3\\envs\\twitter-cpi\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    682\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_dataclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\twitter-cpi\\lib\\site-packages\\snscrape\\modules\\twitter.py\u001b[0m in \u001b[0;36mget_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m                         \u001b[1;32mdel\u001b[0m \u001b[0mpaginationParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_search_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_api_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://api.twitter.com/2/search/adaptive.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaginationParams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_instructions_to_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\twitter-cpi\\lib\\site-packages\\snscrape\\modules\\twitter.py\u001b[0m in \u001b[0;36m_iter_api_data\u001b[1;34m(self, endpoint, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Retrieving scroll page {cursor}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_api_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreqParams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\twitter-cpi\\lib\\site-packages\\snscrape\\modules\\twitter.py\u001b[0m in \u001b[0;36m_get_api_data\u001b[1;34m(self, endpoint, params)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_api_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_guest_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apiHeaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponseOkCallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_api_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\twitter-cpi\\lib\\site-packages\\snscrape\\modules\\twitter.py\u001b[0m in \u001b[0;36m_ensure_guest_token\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    198\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apiHeaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x-guest-token'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_guestToken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0msnscrape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScraperException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unable to find guest token'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_unset_guest_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mScraperException\u001b[0m: Unable to find guest token"
     ]
    }
   ],
   "source": [
    "for week in week_list:\n",
    "    _week = Week(week_number=week['week_number'],\n",
    "                week_start=week['week_start'],\n",
    "                deponents=week['deponents'])\n",
    "    week_tweet_extract(_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2009ccc-a5cc-44c5-9c6a-03345927501b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-cpi",
   "language": "python",
   "name": "twitter-cpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
