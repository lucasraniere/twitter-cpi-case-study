{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c467f6bd-a30f-4509-9dcb-bfd42c3bebec",
   "metadata": {},
   "source": [
    "# __Extracted Tweets Data Analyis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d79ccc-bcbc-4784-aab6-944e1439fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, json, datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4c0ea9-2f40-4c8f-b33d-a6b7054ab5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of extracted weeks:  24\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/tweets/'\n",
    "\n",
    "week_list = [week_dir for week_dir in os.listdir(DATA_PATH) if os.path.isdir(DATA_PATH+week_dir) and not week_dir.endswith('.ipynb_checkpoints')]\n",
    "print('Amount of extracted weeks: ', len(week_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ff40da-03e3-4dcd-9478-c10c7555dd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['week_01',\n",
       " 'week_02',\n",
       " 'week_03',\n",
       " 'week_04',\n",
       " 'week_05',\n",
       " 'week_06',\n",
       " 'week_07',\n",
       " 'week_08',\n",
       " 'week_09',\n",
       " 'week_10',\n",
       " 'week_11',\n",
       " 'week_12',\n",
       " 'week_pr_01',\n",
       " 'week_pr_02',\n",
       " 'week_13',\n",
       " 'week_14',\n",
       " 'week_15',\n",
       " 'week_16',\n",
       " 'week_17',\n",
       " 'week_pr_03',\n",
       " 'week_18',\n",
       " 'week_19',\n",
       " 'week_20',\n",
       " 'week_21']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_list.sort()\n",
    "week_list.remove('week_pr_01')\n",
    "week_list.insert(week_list.index('week_12')+1, 'week_pr_01')\n",
    "week_list.remove('week_pr_02')\n",
    "week_list.insert(week_list.index('week_pr_01')+1, 'week_pr_02')\n",
    "week_list.remove('week_pr_03')\n",
    "week_list.insert(week_list.index('week_17')+1, 'week_pr_03')\n",
    "week_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4eee93-7e14-4442-a23a-6de8a03f6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "tts_amount = {\n",
    "    'week': {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    },\n",
    "    'day': {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    }\n",
    "}\n",
    "frequent_top_10_hashtags = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []\n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    for file in os.listdir(DATA_PATH+week):\n",
    "        if file.endswith('.json'):\n",
    "            with open(f'{DATA_PATH+week}/{file}', encoding='utf-8') as week_info:\n",
    "                data = json.load(week_info)\n",
    "                tts_amount['week']['query_ext'].append(data['tweets_amount']['query_ext'])\n",
    "                tts_amount['week']['hashtag_ext'].append(data['tweets_amount']['hashtag_ext'])\n",
    "                for hashtag in data['top_10_hashtags']['query_ext']:\n",
    "                    frequent_top_10_hashtags['query_ext'].append(hashtag)\n",
    "                for hashtag in data['top_10_hashtags']['hashtag_ext']:\n",
    "                    frequent_top_10_hashtags['hashtag_ext'].append(hashtag)\n",
    "                for day in data['days_info']:\n",
    "                    days.append(f'{week}_{day}')\n",
    "                    tts_amount['day']['query_ext'].append(data['days_info'][day]['tweets_amount']['query_ext'])\n",
    "                    tts_amount['day']['hashtag_ext'].append(data['days_info'][day]['tweets_amount']['hashtag_ext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c37d0-935c-4782-a1ab-a7b6dd15c173",
   "metadata": {},
   "source": [
    "## __Quantitative Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b477f933-53c5-441a-a3c3-d9266a03ca51",
   "metadata": {},
   "source": [
    "### __Tweets Amount__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e15ca7-3e2a-49f0-b521-523faaa88780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cff6b388648489c81caaac9dfeae2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weekly_tts_amount = pd.DataFrame(tts_amount['week'])\n",
    "weekly_tts_amount['week'] = week_list\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(x='week', y='value', hue='dataset', data=pd.melt(weekly_tts_amount, ['week']).rename(columns={'variable':'dataset'}))\n",
    "ax.set(xlabel='week number', ylabel='amount of tweets')\n",
    "plt.title('Tweets amount per week')\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c146feaf-4c02-45f7-82de-049e129b72b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7146bb1cc61e48ba86d1936834917641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "daily_tts_amount = pd.DataFrame(tts_amount['day'])\n",
    "daily_tts_amount['day'] = days\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(x='day', y='value', hue='dataset', data=pd.melt(daily_tts_amount, ['day']).rename(columns={'variable':'dataset'}))\n",
    "ax.set(xlabel='days', ylabel='amount of tweets')\n",
    "plt.title('Tweets amount per day')\n",
    "plt.tick_params(\n",
    "    axis='x',\n",
    "    which='both',\n",
    "    bottom=False,\n",
    "    top=False,\n",
    "    labelbottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd22f3eb-bdc3-484b-996f-1d61a28aee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of weeks: 24\n",
      "\n",
      "Total number of tweets:\n",
      "- Query dataset: 387377\n",
      "- Hashtag dataset: 2958486\n",
      "\n",
      "Weeks with the highest amount of tweets:\n",
      "- Query dataset: \n",
      "[['week_02', 37097], ['week_04', 37037], ['week_03', 36058], ['week_01', 29618], ['week_05', 26749]]\n",
      "- Hashtag dataset: \n",
      "[['week_10', 255016], ['week_09', 246175], ['week_05', 230436], ['week_13', 197249], ['week_11', 192390]]\n",
      "\n",
      "Days with the highest amount of tweets:\n",
      "- Query dataset:\n",
      "[['week_03_day_4', 14333], ['week_02_day_3', 11035], ['week_01_day_3', 10495], ['week_04_day_4', 10262], ['week_04_day_3', 9957]]\n",
      "- Hashtag dataset:\n",
      "[['week_05_day_7', 131549], ['week_10_day_2', 87000], ['week_03_day_5', 83904], ['week_pr_03_day_3', 82177], ['week_08_day_7', 60863]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Total number of weeks: {len(week_list)}\n",
    "\n",
    "Total number of tweets:\n",
    "- Query dataset: {sum(tts_amount['week']['query_ext'])}\n",
    "- Hashtag dataset: {sum(tts_amount['week']['hashtag_ext'])}\n",
    "\n",
    "Weeks with the highest amount of tweets:\n",
    "- Query dataset: \n",
    "{weekly_tts_amount.sort_values(by='query_ext', ascending=False)[:5][['week','query_ext']].values.tolist()}\n",
    "- Hashtag dataset: \n",
    "{weekly_tts_amount.sort_values(by='hashtag_ext', ascending=False)[:5][['week','hashtag_ext']].values.tolist()}\n",
    "\n",
    "Days with the highest amount of tweets:\n",
    "- Query dataset:\n",
    "{daily_tts_amount.sort_values(by='query_ext', ascending=False)[:5][['day', 'query_ext']].values.tolist()}\n",
    "- Hashtag dataset:\n",
    "{daily_tts_amount.sort_values(by='hashtag_ext', ascending=False)[:5][['day', 'hashtag_ext']].values.tolist()}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741374cb-1356-4da7-b79b-04520ab848df",
   "metadata": {},
   "source": [
    "### __User Amount__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92eae57-59c3-4673-93ea-7f4fdf3d90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lists = {}\n",
    "user_count = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []    \n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    query_users = []\n",
    "    hashtag_users = []\n",
    "    file_lists[week] = {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    }\n",
    "    for file in os.listdir(DATA_PATH+week):\n",
    "        if file.endswith('query_ext.parquet'):\n",
    "            file_lists[week]['query_ext'].append(file)\n",
    "        if file.endswith('hashtags_ext.parquet'):\n",
    "            file_lists[week]['hashtag_ext'].append(file)\n",
    "    file_lists[week]['query_ext'].sort()\n",
    "    file_lists[week]['hashtag_ext'].sort()\n",
    "    \n",
    "    for file in file_lists[week]['query_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for user in df['user']:\n",
    "            query_users.append(eval(user)['username'])\n",
    "    user_count['query_ext'].append(len(set(query_users)))\n",
    "    for file in file_lists[week]['hashtag_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for user in df['user']:\n",
    "            hashtag_users.append(eval(user)['username'])\n",
    "    user_count['hashtag_ext'].append(len(set(hashtag_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e537d4-aeca-46ae-bf81-a67403bdf260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ca54ab7c8643ba99567d3ff76a2466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_count_df = pd.DataFrame(user_count)\n",
    "user_count_df['week'] = week_list\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(x='week', y='value', hue='dataset', data=pd.melt(user_count_df, ['week']).rename(columns={'variable':'dataset'}))\n",
    "ax.set(xlabel='week number', ylabel='amount of users')\n",
    "plt.title('Users amount per week')\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c08ee7-600c-4d89-8ded-00eb4177bbcc",
   "metadata": {},
   "source": [
    "## __Qualitative Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859f8ae-25c5-441f-a69a-398985175686",
   "metadata": {},
   "source": [
    "### __Hashtag Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95bcac-02ff-47b2-bb7a-476344fdbe75",
   "metadata": {},
   "source": [
    "#### Most frequent hashtags in the Top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c01d2e-3415-467e-a0e8-825535310d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#forabolsonaro            20\n",
       "#cpi                      18\n",
       "#cpidapandemia            18\n",
       "#cpidacovid               16\n",
       "#forabolsonarogenocida    15\n",
       "#pandemia                 13\n",
       "#covid                    12\n",
       "#cpidocirco               12\n",
       "#covid19                  10\n",
       "#brasil                   10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(frequent_top_10_hashtags['query_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3078156e-f034-49f8-adcc-254e365332f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#forabolsonaro            6\n",
       "#cpidapandemia            5\n",
       "#cpidacovid19             4\n",
       "#impeachmentja            4\n",
       "#vacinassalvamvidas       4\n",
       "#fakenews                 4\n",
       "#forabol卐onarogenocida    4\n",
       "#bolsonaro2022            4\n",
       "#12setforabolsonaro       3\n",
       "#vacina                   3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(frequent_top_10_hashtags['hashtag_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190730df-c95c-4e06-9685-1bb29ecf202f",
   "metadata": {},
   "source": [
    "#### Most frequent hashtags throughout the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f1f3789-76f1-46f1-92d8-cfb96d028f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []\n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    for file in file_lists[week]['query_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for hashtag_list in df['hashtags']:\n",
    "            hashtag_list = eval(hashtag_list)\n",
    "            if hashtag_list:\n",
    "                for hashtag in hashtag_list:\n",
    "                    hashtags['query_ext'].append(hashtag.lower())\n",
    "    for file in file_lists[week]['hashtag_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for hashtag_list in df['hashtags']:\n",
    "            hashtag_list = eval(hashtag_list)\n",
    "            if hashtag_list:\n",
    "                for hashtag in hashtag_list:\n",
    "                    hashtags['hashtag_ext'].append(hashtag.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2357045a-b0c2-4352-ae3f-a143d35d2def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpidacovid        9839\n",
       "cpidapandemia     4109\n",
       "cpi               2928\n",
       "forabolsonaro     2047\n",
       "cpidocirco        1460\n",
       "globonews         1350\n",
       "covid             1113\n",
       "cpidogenocidio    1084\n",
       "pandemia          1078\n",
       "covid19           1021\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hashtags['query_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9581585-522c-4914-86af-1904dfb2d1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpidacovid               597552\n",
       "forabolsonaro            580075\n",
       "forabolsonarogenocida    147935\n",
       "cpidapandemia            140279\n",
       "renanvagabundo           115554\n",
       "cpidocirco               107150\n",
       "renansabiadetudo          88848\n",
       "29mforabolsonaro          79660\n",
       "euautorizopresidente      79111\n",
       "cpidotse                  56165\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hashtags['hashtag_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83baa7ab-9932-4621-9cb0-d799ea9de7ff",
   "metadata": {},
   "source": [
    "### __Topic Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934ade0c-d05f-4b53-8f77-07126dcce15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e86450-a7d6-4e58-8358-7406af530af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []\n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    for file in file_lists[week]['query_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for tweet in df['content']:\n",
    "            docs['query_ext'].append(tweet)\n",
    "    for file in file_lists[week]['hashtag_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for tweet in df['content']:\n",
    "            docs['hashtag_ext'].append(tweet)\n",
    "   \n",
    "# removing duplicated tweets\n",
    "for key in docs:\n",
    "    tweets_series = pd.Series(docs[key])\n",
    "    tweets_series.drop_duplicates(inplace=True)\n",
    "    docs[key] = tweets_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f90473-2584-4423-a6b4-d2234ca51c0e",
   "metadata": {},
   "source": [
    "#### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf2bd7b-cdc6-4106-a40c-4d60623a6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "stop_words.append('pra')\n",
    "stop_words.append('tá')\n",
    "stop_words.append('sobre')\n",
    "\n",
    "def remove_emoji(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "def remove_link(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)\n",
    "    tweet = tweet.strip('[link]')\n",
    "    tweet = re.sub(r'pic.twitter\\S+', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_av(tweet):\n",
    "    tweet = re.sub(r'VIDEO:', '', tweet)\n",
    "    tweet = re.sub(r'AUDIO:', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def tweet_preproc(tweet):\n",
    "    '''\n",
    "    Remove: @mentions, #hashtags, URL\n",
    "    links, punctuation and emojis,\n",
    "    and multiple white spaces\n",
    "    '''\n",
    "    tweet = remove_emoji(tweet)\n",
    "    tweet = remove_link(tweet)\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet = re.sub(r'\\d', '', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = [word for word in tweet.split() if len(word)>2 and word not in stop_words]\n",
    "    \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14e732ec-3ab1-4791-9678-d9bedaa2b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in docs:\n",
    "    doc = []\n",
    "    for tweet in docs[key]:\n",
    "        doc.append(tweet_preproc(tweet))\n",
    "    docs[key] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e29c4e-34e7-4d6a-a315-bb41b472db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amount of tweets\n",
      "- Query dataset: 384798\n",
      "- Hashtag dataset: 2917027\n",
      "\n"
     ]
    }
   ],
   "source": [
    " print(f'''\n",
    "Amount of tweets\n",
    "- Query dataset: {len(docs['query_ext'])}\n",
    "- Hashtag dataset: {len(docs['hashtag_ext'])}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63d7f093-db98-4920-9209-c645bda516e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpi',\n",
       " 'covid',\n",
       " 'faz',\n",
       " 'rir',\n",
       " 'jovem',\n",
       " 'bando',\n",
       " 'corrupto',\n",
       " 'vai',\n",
       " 'fazer',\n",
       " 'alguma',\n",
       " 'coisa',\n",
       " 'além',\n",
       " 'proselitismo',\n",
       " 'político',\n",
       " 'estude',\n",
       " 'definição',\n",
       " 'genocídio',\n",
       " 'pare',\n",
       " 'passar',\n",
       " 'vergonha',\n",
       " 'cairão',\n",
       " 'lado',\n",
       " 'mil',\n",
       " 'direita',\n",
       " 'serás',\n",
       " 'atingindo']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs['query_ext'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f922554a-5d89-4154-ba38-e86d9fbb3231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fique', 'certo', 'extrema', 'esquerda', 'vai', 'ser', 'contra']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs['hashtag_ext'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37abed-a55c-4971-9906-e01bcdecd465",
   "metadata": {},
   "source": [
    "#### Train STTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b99489e9-0e79-4513-8a0b-44b9799610ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm import MovieGroupProcess # loading gsdmm model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183cd15-5d84-402c-ac3d-3df33297fa9e",
   "metadata": {},
   "source": [
    "### __QUERY DATASET__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "203076f4-ba0f-4e01-a6ba-7719ad859219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY DATASET\n",
      "Vocabulary size: 126808\n",
      "Number of documents: 384798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = set(x for doc in docs['query_ext'] for x in doc)\n",
    "n_terms = len(vocab)\n",
    "\n",
    "print(f'''\n",
    "QUERY DATASET\n",
    "Vocabulary size: {n_terms}\n",
    "Number of documents: {len(docs['query_ext'])}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfd2c60c-89ae-46b6-8864-0b7eb285c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 345091 clusters with 15 clusters populated\n",
      "In stage 1: transferred 246673 clusters with 15 clusters populated\n",
      "In stage 2: transferred 152354 clusters with 15 clusters populated\n",
      "In stage 3: transferred 112733 clusters with 15 clusters populated\n",
      "In stage 4: transferred 90144 clusters with 15 clusters populated\n",
      "In stage 5: transferred 78185 clusters with 15 clusters populated\n",
      "In stage 6: transferred 74144 clusters with 15 clusters populated\n",
      "In stage 7: transferred 71833 clusters with 15 clusters populated\n",
      "In stage 8: transferred 70263 clusters with 15 clusters populated\n",
      "In stage 9: transferred 69260 clusters with 15 clusters populated\n",
      "In stage 10: transferred 68107 clusters with 15 clusters populated\n",
      "In stage 11: transferred 67391 clusters with 15 clusters populated\n",
      "In stage 12: transferred 67265 clusters with 15 clusters populated\n",
      "In stage 13: transferred 67129 clusters with 15 clusters populated\n",
      "In stage 14: transferred 66674 clusters with 15 clusters populated\n",
      "In stage 15: transferred 66198 clusters with 15 clusters populated\n",
      "In stage 16: transferred 65692 clusters with 15 clusters populated\n",
      "In stage 17: transferred 65670 clusters with 15 clusters populated\n",
      "In stage 18: transferred 65546 clusters with 15 clusters populated\n",
      "In stage 19: transferred 65094 clusters with 15 clusters populated\n",
      "In stage 20: transferred 64959 clusters with 15 clusters populated\n",
      "In stage 21: transferred 64817 clusters with 15 clusters populated\n",
      "In stage 22: transferred 64817 clusters with 15 clusters populated\n",
      "In stage 23: transferred 64500 clusters with 15 clusters populated\n",
      "In stage 24: transferred 64313 clusters with 15 clusters populated\n",
      "In stage 25: transferred 64023 clusters with 15 clusters populated\n",
      "In stage 26: transferred 63656 clusters with 15 clusters populated\n",
      "In stage 27: transferred 63940 clusters with 15 clusters populated\n",
      "In stage 28: transferred 63647 clusters with 15 clusters populated\n",
      "In stage 29: transferred 63631 clusters with 15 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp = MovieGroupProcess(K=15, alpha=0.10, beta=0.10, n_iters=30)\n",
    "\n",
    "y = mgp.fit(docs['query_ext'], n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d1b0793-20e7-4b35-918b-7f11fb1615af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5884314e-a850-48d7-99fb-f739b05af2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic:  [51568 14770  2105 29863 49413 24082 29603 17869 32305 31697 30679  5934\n",
      " 22207 11176 31527]\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents per topic: ', doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03a7780b-fd15-4797-964a-6d685ff65bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important clusters (by number of docs inside): [ 0  4  8  9 14 10  3  6  5 12  7  1 13 11  2]\n"
     ]
    }
   ],
   "source": [
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42f7a913-08b4-4b7b-97ea-fda02417c3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0 : [('cpi', 55228), ('covid', 41397), ('pandemia', 11449), ('vai', 10150), ('bolsonaro', 6946), ('ser', 4782), ('nada', 4482), ('presidente', 4031), ('cara', 3861), ('governo', 3622), ('agora', 3389), ('tudo', 3310), ('gente', 3294), ('ver', 3135), ('brasil', 2928)]\n",
      "\n",
      "Cluster 4 : [('cpi', 50455), ('covid', 43434), ('pandemia', 6268), ('hoje', 4243), ('dia', 3637), ('assistir', 3093), ('bbb', 2872), ('ver', 2700), ('vai', 2698), ('melhor', 2666), ('gente', 2276), ('assistindo', 2251), ('entretenimento', 2082), ('brasil', 1713), ('vendo', 1673)]\n",
      "\n",
      "Cluster 8 : [('cpi', 36071), ('covid', 17342), ('pandemia', 16478), ('governadores', 7808), ('dinheiro', 6883), ('prefeitos', 4581), ('governo', 4476), ('investigar', 4454), ('presidente', 4413), ('federal', 4029), ('circo', 4000), ('stf', 3314), ('combate', 3300), ('povo', 3130), ('ser', 2982)]\n",
      "\n",
      "Cluster 9 : [('cpi', 34124), ('covid', 22220), ('pandemia', 11076), ('bolsonaro', 7954), ('governo', 5311), ('presidente', 3668), ('contra', 3205), ('vai', 2891), ('brasil', 2879), ('ser', 2471), ('stf', 2386), ('senado', 2142), ('impeachment', 1850), ('crimes', 1739), ('pode', 1593)]\n",
      "\n",
      "Cluster 14 : [('cpi', 33424), ('covid', 24077), ('pandemia', 7913), ('senador', 7173), ('senadores', 3592), ('hoje', 2178), ('ser', 2170), ('parabéns', 2110), ('presidente', 1995), ('dra', 1757), ('marcos', 1721), ('rogério', 1714), ('brasil', 1684), ('nise', 1628), ('vergonha', 1585)]\n",
      "\n",
      "Cluster 10 : [('cpi', 32477), ('covid', 22272), ('pandemia', 9890), ('senador', 4975), ('bolsonaro', 4668), ('presidente', 3419), ('via', 3248), ('comissão', 3234), ('senado', 2805), ('governo', 2750), ('senadores', 2739), ('randolfe', 1985), ('diz', 1930), ('convocação', 1828), ('renan', 1785)]\n",
      "\n",
      "Cluster 3 : [('cpi', 30713), ('covid', 23444), ('pandemia', 9932), ('governo', 5182), ('bolsonaro', 4585), ('brasil', 4343), ('vacina', 3717), ('mortes', 3684), ('vacinas', 3397), ('mil', 3363), ('cloroquina', 2698), ('contra', 2615), ('ser', 2576), ('saúde', 2535), ('ter', 2500)]\n",
      "\n",
      "Cluster 6 : [('cpi', 30875), ('covid', 23184), ('pandemia', 7267), ('saúde', 6398), ('ouve', 4917), ('depoimento', 4490), ('covaxin', 3823), ('ministério', 3653), ('vivo', 3401), ('bolsonaro', 3280), ('governo', 3205), ('compra', 2962), ('vacina', 2953), ('vacinas', 2907), ('nesta', 2814)]\n",
      "\n",
      "Cluster 5 : [('cpi', 25215), ('covid', 20853), ('pazuello', 7793), ('bolsonaro', 5034), ('depoimento', 4442), ('pandemia', 4203), ('luciano', 2695), ('diz', 2409), ('depor', 2393), ('saúde', 2312), ('hang', 2115), ('vai', 2081), ('renan', 2028), ('queiroga', 1982), ('via', 1940)]\n",
      "\n",
      "Cluster 12 : [('cpi', 22686), ('covid', 19801), ('cloroquina', 5157), ('pandemia', 4754), ('prevent', 3762), ('bolsonaro', 3621), ('saúde', 3243), ('diz', 3183), ('senior', 2945), ('tratamento', 2764), ('depoimento', 2711), ('contra', 2417), ('médica', 2247), ('presidente', 2095), ('ouve', 2038)]\n",
      "\n",
      "Cluster 7 : [('cpi', 18692), ('covid', 14514), ('stf', 4811), ('pandemia', 3571), ('sigilo', 2948), ('quebra', 2778), ('pazuello', 2633), ('governadores', 2376), ('silêncio', 2286), ('carlos', 1879), ('convocação', 1805), ('via', 1748), ('wizard', 1737), ('ficar', 1686), ('bolsonaro', 1561)]\n",
      "\n",
      "Cluster 1 : [('cpi', 16036), ('covid', 12193), ('renan', 9571), ('calheiros', 6749), ('relator', 4078), ('bolsonaro', 2708), ('pandemia', 2654), ('presidente', 2385), ('senador', 2172), ('relatoria', 1942), ('ser', 1565), ('aziz', 1465), ('omar', 1441), ('stf', 1279), ('via', 1182)]\n",
      "\n",
      "Cluster 13 : [('cpi', 11627), ('covid', 9092), ('pandemia', 2675), ('bolsonaro', 2219), ('news', 1415), ('fake', 1385), ('governo', 1135), ('via', 1086), ('presidente', 1011), ('senadores', 937), ('forças', 848), ('nota', 820), ('armadas', 810), ('contra', 809), ('globo', 785)]\n",
      "\n",
      "Cluster 11 : [('cpi', 6390), ('covid', 4729), ('presidente', 2933), ('aziz', 2283), ('omar', 2253), ('pandemia', 1240), ('senador', 971), ('bolsonaro', 946), ('saúde', 935), ('vídeo', 714), ('esposa', 694), ('investigado', 658), ('via', 568), ('brasil', 563), ('pedofilia', 560)]\n",
      "\n",
      "Cluster 2 : [('cpi', 3769), ('covid', 2861), ('anexo', 1438), ('link', 1392), ('pandemia', 768), ('democracia', 712), ('vai', 690), ('pizza', 632), ('poder', 602), ('informacional', 477), ('combate', 448), ('brasil', 408), ('quarto', 371), ('acabar', 345), ('ser', 303)]\n"
     ]
    }
   ],
   "source": [
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(mgp.cluster_word_distribution, top_index, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f13e6-dbce-42f1-8069-5b7456dc5e2b",
   "metadata": {},
   "source": [
    "### __HASHTAG DATASET__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d328494-3d29-4011-9ce4-b815f2fa9dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HASHTAG DATASET\n",
      "Vocabulary size: 388370\n",
      "Number of documents: 2917027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = set(x for doc in docs['hashtag_ext'] for x in doc)\n",
    "n_terms = len(vocab)\n",
    "\n",
    "print(f'''\n",
    "HASHTAG DATASET\n",
    "Vocabulary size: {n_terms}\n",
    "Number of documents: {len(docs['hashtag_ext'])}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0d38b00-580e-4f06-b8ee-05f73ae85829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 2675100 clusters with 15 clusters populated\n",
      "In stage 1: transferred 2407687 clusters with 15 clusters populated\n",
      "In stage 2: transferred 1861902 clusters with 15 clusters populated\n",
      "In stage 3: transferred 1411733 clusters with 15 clusters populated\n",
      "In stage 4: transferred 1209460 clusters with 15 clusters populated\n",
      "In stage 5: transferred 1127918 clusters with 15 clusters populated\n",
      "In stage 6: transferred 1088881 clusters with 15 clusters populated\n",
      "In stage 7: transferred 1065805 clusters with 15 clusters populated\n",
      "In stage 8: transferred 1049648 clusters with 15 clusters populated\n",
      "In stage 9: transferred 1037996 clusters with 15 clusters populated\n",
      "In stage 10: transferred 1026816 clusters with 15 clusters populated\n",
      "In stage 11: transferred 1016918 clusters with 15 clusters populated\n",
      "In stage 12: transferred 1006848 clusters with 15 clusters populated\n",
      "In stage 13: transferred 997350 clusters with 15 clusters populated\n",
      "In stage 14: transferred 989140 clusters with 15 clusters populated\n",
      "In stage 15: transferred 981357 clusters with 15 clusters populated\n",
      "In stage 16: transferred 976193 clusters with 15 clusters populated\n",
      "In stage 17: transferred 972397 clusters with 15 clusters populated\n",
      "In stage 18: transferred 968866 clusters with 15 clusters populated\n",
      "In stage 19: transferred 966701 clusters with 15 clusters populated\n",
      "In stage 20: transferred 964379 clusters with 15 clusters populated\n",
      "In stage 21: transferred 962301 clusters with 15 clusters populated\n",
      "In stage 22: transferred 960998 clusters with 15 clusters populated\n",
      "In stage 23: transferred 959160 clusters with 15 clusters populated\n",
      "In stage 24: transferred 957234 clusters with 15 clusters populated\n",
      "In stage 25: transferred 956461 clusters with 15 clusters populated\n",
      "In stage 26: transferred 956031 clusters with 15 clusters populated\n",
      "In stage 27: transferred 955162 clusters with 15 clusters populated\n",
      "In stage 28: transferred 954176 clusters with 15 clusters populated\n",
      "In stage 29: transferred 952959 clusters with 15 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp = MovieGroupProcess(K=15, alpha=0.10, beta=0.10, n_iters=30)\n",
    "\n",
    "y = mgp.fit(docs['hashtag_ext'], n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b80b18b3-7e52-43d0-ba2a-40da71776435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58b7c41a-9dfc-4e46-b704-2fba9cec17bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic:  [228136 140719 150552 237787 119179 234885 291932 326260 390294  79393\n",
      "  56777 377937  18568 191352  73256]\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents per topic: ', doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8459b5f-7e5c-4b95-b6c9-954404f4ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important clusters (by number of docs inside): [ 8 11  7  6  3  5  0 13  2  1  4  9 14 10 12]\n"
     ]
    }
   ],
   "source": [
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96c6ed83-8bd2-43b4-ba70-564f7ca467ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 8 : [('senador', 36434), ('cpi', 30336), ('vai', 30135), ('marcos', 25608), ('rogério', 23999), ('cara', 22924), ('hoje', 21359), ('omar', 15818), ('falar', 14535), ('agora', 14159), ('ser', 13478), ('gente', 13426), ('fala', 12947), ('renan', 11604), ('heinze', 10882)]\n",
      "\n",
      "Cluster 11 : [('vai', 45936), ('bolsonaro', 44919), ('vagabundo', 21019), ('ser', 20202), ('genocida', 18383), ('ladrão', 17004), ('presidente', 15387), ('brasil', 12316), ('agora', 12014), ('corrupto', 12010), ('lula', 11832), ('cadeia', 11390), ('cara', 11097), ('pode', 10785), ('renan', 10561)]\n",
      "\n",
      "Cluster 7 : [('vai', 18528), ('hoje', 17046), ('gente', 16781), ('aqui', 14909), ('brasil', 14652), ('bolsonaro', 14131), ('dia', 12621), ('agora', 12447), ('vou', 11434), ('ser', 11167), ('ver', 10929), ('ainda', 8510), ('fazer', 8355), ('tudo', 8300), ('bem', 8201)]\n",
      "\n",
      "Cluster 6 : [('bolsonaro', 39459), ('brasil', 35725), ('governo', 30356), ('povo', 23235), ('mil', 23061), ('país', 22456), ('genocida', 20821), ('presidente', 20385), ('ser', 18505), ('pessoas', 15904), ('vai', 14630), ('vacina', 14324), ('jforabolsonaro', 13508), ('gente', 13310), ('ainda', 12682)]\n",
      "\n",
      "Cluster 3 : [('voto', 33400), ('povo', 28799), ('bolsonaro', 25887), ('presidente', 18411), ('contra', 16650), ('brasil', 16373), ('ser', 15381), ('vai', 14281), ('impresso', 14007), ('stf', 13347), ('democracia', 10791), ('auditável', 10391), ('quer', 9705), ('eleições', 9662), ('vamos', 9175)]\n",
      "\n",
      "Cluster 5 : [('cpi', 43541), ('vergonha', 17980), ('circo', 14510), ('povo', 13877), ('senadores', 12372), ('presidente', 12213), ('brasil', 11637), ('ser', 11539), ('senador', 11315), ('dinheiro', 10653), ('vai', 10556), ('todos', 10153), ('gabas', 8812), ('renan', 8737), ('governo', 8318)]\n",
      "\n",
      "Cluster 0 : [('dia', 57257), ('mforabolsonaro', 40180), ('bom', 33483), ('jforabolsonaro', 27059), ('hoje', 23515), ('bolsonaro', 18577), ('mpovonasruas', 15925), ('vai', 15734), ('ruas', 13283), ('vamos', 12783), ('brasil', 12264), ('povo', 11743), ('manifestação', 9890), ('todos', 9793), ('genocida', 9705)]\n",
      "\n",
      "Cluster 13 : [('presidente', 20357), ('brasil', 20156), ('vamos', 18489), ('bora', 14118), ('todos', 11570), ('boa', 11468), ('deus', 11329), ('bolsonaro', 10976), ('povo', 7942), ('dia', 6951), ('tag', 6856), ('parabéns', 6488), ('verdade', 6307), ('acima', 6158), ('sempre', 6092)]\n",
      "\n",
      "Cluster 2 : [('cpi', 22958), ('bolsonaro', 21612), ('governo', 15773), ('saúde', 11718), ('presidente', 11496), ('vacina', 10291), ('pazuello', 9015), ('vacinas', 8856), ('senador', 8061), ('diz', 7926), ('ministério', 7824), ('depoimento', 7525), ('covid', 7518), ('ser', 7389), ('vai', 7306)]\n",
      "\n",
      "Cluster 1 : [('cloroquina', 16448), ('covid', 13675), ('tratamento', 11626), ('cpi', 10218), ('senador', 9766), ('saúde', 9229), ('ser', 8833), ('bolsonaro', 8226), ('precoce', 8131), ('governo', 7906), ('pandemia', 6654), ('contra', 6366), ('vacina', 6130), ('brasil', 6106), ('presidente', 6078)]\n",
      "\n",
      "Cluster 4 : [('jforabolsonaro', 46409), ('bolsonaro', 23726), ('jpovonasruas', 15093), ('mforabolsonaro', 11438), ('brasil', 10424), ('contra', 10227), ('ruas', 10111), ('povo', 9491), ('vacina', 8837), ('genocida', 8763), ('ato', 7557), ('governo', 7152), ('rio', 6799), ('dia', 6542), ('hoje', 6191)]\n",
      "\n",
      "Cluster 9 : [('covid', 10235), ('brasil', 9823), ('contra', 7774), ('casos', 6396), ('milhões', 5066), ('reforma', 5062), ('mortes', 4917), ('público', 4831), ('pandemia', 4567), ('governo', 4521), ('pec', 4358), ('doses', 4330), ('administrativa', 4301), ('saúde', 4118), ('vacina', 4080)]\n",
      "\n",
      "Cluster 14 : [('vacina', 12746), ('dose', 10246), ('viva', 7913), ('sus', 6296), ('dia', 5306), ('vacinada', 5268), ('hoje', 4992), ('segunda', 4179), ('primeira', 4115), ('feliz', 3898), ('vacinado', 3548), ('deus', 3364), ('tomar', 3178), ('ciência', 3094), ('covid', 2964)]\n",
      "\n",
      "Cluster 10 : [('rede', 68532), ('núcleo', 47857), ('luciano', 47815), ('lepera', 47034), ('direitos', 21898), ('curta', 21477), ('inscrevase', 14139), ('jforabolsonaro', 12926), ('réuconfesso', 12258), ('bolsonaro', 10039), ('sforabolsonaro', 7550), ('fortaleça', 7163), ('outforabolsonaro', 5951), ('tácaroculpadobolsonaro', 5670), ('diz', 4002)]\n",
      "\n",
      "Cluster 12 : [('bolsonaro', 3459), ('impeachment', 3123), ('jair', 2794), ('dep', 2484), ('apoie', 2379), ('juliette', 1885), ('blá', 1485), ('fogo', 1073), ('pode', 1065), ('favor', 960), ('juiz', 914), ('abrir', 903), ('pega', 870), ('gil', 772), ('cruzeiro', 743)]\n"
     ]
    }
   ],
   "source": [
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(mgp.cluster_word_distribution, top_index, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f161ef14-5b60-41c7-907a-c95e0ae32e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-cpi",
   "language": "python",
   "name": "twitter-cpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
