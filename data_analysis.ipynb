{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c467f6bd-a30f-4509-9dcb-bfd42c3bebec",
   "metadata": {},
   "source": [
    "# __Extracted Tweets Data Analyis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d79ccc-bcbc-4784-aab6-944e1439fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, json, datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4c0ea9-2f40-4c8f-b33d-a6b7054ab5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of extracted weeks:  24\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/tweets/'\n",
    "\n",
    "week_list = [week_dir for week_dir in os.listdir(DATA_PATH) if os.path.isdir(DATA_PATH+week_dir) and not week_dir.endswith('.ipynb_checkpoints')]\n",
    "print('Amount of extracted weeks: ', len(week_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ff40da-03e3-4dcd-9478-c10c7555dd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['week_01',\n",
       " 'week_02',\n",
       " 'week_03',\n",
       " 'week_04',\n",
       " 'week_05',\n",
       " 'week_06',\n",
       " 'week_07',\n",
       " 'week_08',\n",
       " 'week_09',\n",
       " 'week_10',\n",
       " 'week_11',\n",
       " 'week_12',\n",
       " 'week_pr_01',\n",
       " 'week_pr_02',\n",
       " 'week_13',\n",
       " 'week_14',\n",
       " 'week_15',\n",
       " 'week_16',\n",
       " 'week_17',\n",
       " 'week_pr_03',\n",
       " 'week_18',\n",
       " 'week_19',\n",
       " 'week_20',\n",
       " 'week_21']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_list.sort()\n",
    "week_list.remove('week_pr_01')\n",
    "week_list.insert(week_list.index('week_12')+1, 'week_pr_01')\n",
    "week_list.remove('week_pr_02')\n",
    "week_list.insert(week_list.index('week_pr_01')+1, 'week_pr_02')\n",
    "week_list.remove('week_pr_03')\n",
    "week_list.insert(week_list.index('week_17')+1, 'week_pr_03')\n",
    "week_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4eee93-7e14-4442-a23a-6de8a03f6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "tts_amount = {\n",
    "    'week': {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    },\n",
    "    'day': {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    }\n",
    "}\n",
    "frequent_top_10_hashtags = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []\n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    for file in os.listdir(DATA_PATH+week):\n",
    "        if file.endswith('.json'):\n",
    "            with open(f'{DATA_PATH+week}/{file}', encoding='utf-8') as week_info:\n",
    "                data = json.load(week_info)\n",
    "                tts_amount['week']['query_ext'].append(data['tweets_amount']['query_ext'])\n",
    "                tts_amount['week']['hashtag_ext'].append(data['tweets_amount']['hashtag_ext'])\n",
    "                for hashtag in data['top_10_hashtags']['query_ext']:\n",
    "                    frequent_top_10_hashtags['query_ext'].append(hashtag)\n",
    "                for hashtag in data['top_10_hashtags']['hashtag_ext']:\n",
    "                    frequent_top_10_hashtags['hashtag_ext'].append(hashtag)\n",
    "                for day in data['days_info']:\n",
    "                    days.append(f'{week}_{day}')\n",
    "                    tts_amount['day']['query_ext'].append(data['days_info'][day]['tweets_amount']['query_ext'])\n",
    "                    tts_amount['day']['hashtag_ext'].append(data['days_info'][day]['tweets_amount']['hashtag_ext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c37d0-935c-4782-a1ab-a7b6dd15c173",
   "metadata": {},
   "source": [
    "## __Quantitative Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b477f933-53c5-441a-a3c3-d9266a03ca51",
   "metadata": {},
   "source": [
    "### __Tweets Amount__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e15ca7-3e2a-49f0-b521-523faaa88780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d8040d0361416cb78a8c71c722dbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weekly_tts_amount = pd.DataFrame(tts_amount['week'])\n",
    "weekly_tts_amount['week'] = week_list\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(x='week', y='value', hue='dataset', data=pd.melt(weekly_tts_amount, ['week']).rename(columns={'variable':'dataset'}))\n",
    "ax.set(xlabel='week number', ylabel='amount of tweets')\n",
    "plt.title('Tweets amount per week')\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c146feaf-4c02-45f7-82de-049e129b72b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188fbb72d88c43cda699d1dd5739e976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "daily_tts_amount = pd.DataFrame(tts_amount['day'])\n",
    "daily_tts_amount['day'] = days\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(x='day', y='value', hue='dataset', data=pd.melt(daily_tts_amount, ['day']).rename(columns={'variable':'dataset'}))\n",
    "ax.set(xlabel='days', ylabel='amount of tweets')\n",
    "plt.title('Tweets amount per day')\n",
    "plt.tick_params(\n",
    "    axis='x',\n",
    "    which='both',\n",
    "    bottom=False,\n",
    "    top=False,\n",
    "    labelbottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd22f3eb-bdc3-484b-996f-1d61a28aee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of weeks: 24\n",
      "\n",
      "Total number of tweets:\n",
      "- Query dataset: 387377\n",
      "- Hashtag dataset: 2958486\n",
      "\n",
      "Weeks with the highest amount of tweets:\n",
      "- Query dataset: \n",
      "[['week_02', 37097], ['week_04', 37037], ['week_03', 36058], ['week_01', 29618], ['week_05', 26749]]\n",
      "- Hashtag dataset: \n",
      "[['week_10', 255016], ['week_09', 246175], ['week_05', 230436], ['week_13', 197249], ['week_11', 192390]]\n",
      "\n",
      "Days with the highest amount of tweets:\n",
      "- Query dataset:\n",
      "[['week_03_day_4', 14333], ['week_02_day_3', 11035], ['week_01_day_3', 10495], ['week_04_day_4', 10262], ['week_04_day_3', 9957]]\n",
      "- Hashtag dataset:\n",
      "[['week_05_day_7', 131549], ['week_10_day_2', 87000], ['week_03_day_5', 83904], ['week_pr_03_day_3', 82177], ['week_08_day_7', 60863]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Total number of weeks: {len(week_list)}\n",
    "\n",
    "Total number of tweets:\n",
    "- Query dataset: {sum(tts_amount['week']['query_ext'])}\n",
    "- Hashtag dataset: {sum(tts_amount['week']['hashtag_ext'])}\n",
    "\n",
    "Weeks with the highest amount of tweets:\n",
    "- Query dataset: \n",
    "{weekly_tts_amount.sort_values(by='query_ext', ascending=False)[:5][['week','query_ext']].values.tolist()}\n",
    "- Hashtag dataset: \n",
    "{weekly_tts_amount.sort_values(by='hashtag_ext', ascending=False)[:5][['week','hashtag_ext']].values.tolist()}\n",
    "\n",
    "Days with the highest amount of tweets:\n",
    "- Query dataset:\n",
    "{daily_tts_amount.sort_values(by='query_ext', ascending=False)[:5][['day', 'query_ext']].values.tolist()}\n",
    "- Hashtag dataset:\n",
    "{daily_tts_amount.sort_values(by='hashtag_ext', ascending=False)[:5][['day', 'hashtag_ext']].values.tolist()}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741374cb-1356-4da7-b79b-04520ab848df",
   "metadata": {},
   "source": [
    "### __User Amount__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92eae57-59c3-4673-93ea-7f4fdf3d90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lists = {}\n",
    "user_count = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []    \n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    query_users = []\n",
    "    hashtag_users = []\n",
    "    file_lists[week] = {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    }\n",
    "    for file in os.listdir(DATA_PATH+week):\n",
    "        if file.endswith('query_ext.parquet'):\n",
    "            file_lists[week]['query_ext'].append(file)\n",
    "        if file.endswith('hashtags_ext.parquet'):\n",
    "            file_lists[week]['hashtag_ext'].append(file)\n",
    "    file_lists[week]['query_ext'].sort()\n",
    "    file_lists[week]['hashtag_ext'].sort()\n",
    "    \n",
    "    for file in file_lists[week]['query_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for user in df['user']:\n",
    "            query_users.append(eval(user)['username'])\n",
    "    user_count['query_ext'].append(len(set(query_users)))\n",
    "    for file in file_lists[week]['hashtag_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for user in df['user']:\n",
    "            hashtag_users.append(eval(user)['username'])\n",
    "    user_count['hashtag_ext'].append(len(set(hashtag_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e537d4-aeca-46ae-bf81-a67403bdf260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded1041c76074c2f843f19317e043191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_count_df = pd.DataFrame(user_count)\n",
    "user_count_df['week'] = week_list\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(x='week', y='value', hue='dataset', data=pd.melt(user_count_df, ['week']).rename(columns={'variable':'dataset'}))\n",
    "ax.set(xlabel='week number', ylabel='amount of users')\n",
    "plt.title('Users amount per week')\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c08ee7-600c-4d89-8ded-00eb4177bbcc",
   "metadata": {},
   "source": [
    "## __Qualitative Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859f8ae-25c5-441f-a69a-398985175686",
   "metadata": {},
   "source": [
    "### __Hashtag Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95bcac-02ff-47b2-bb7a-476344fdbe75",
   "metadata": {},
   "source": [
    "#### Most frequent hashtags in the Top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c01d2e-3415-467e-a0e8-825535310d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#forabolsonaro            20\n",
       "#cpi                      18\n",
       "#cpidapandemia            18\n",
       "#cpidacovid               16\n",
       "#forabolsonarogenocida    15\n",
       "#pandemia                 13\n",
       "#covid                    12\n",
       "#cpidocirco               12\n",
       "#covid19                  10\n",
       "#brasil                   10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(frequent_top_10_hashtags['query_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3078156e-f034-49f8-adcc-254e365332f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#forabolsonaro            6\n",
       "#cpidapandemia            5\n",
       "#cpidacovid19             4\n",
       "#impeachmentja            4\n",
       "#vacinassalvamvidas       4\n",
       "#fakenews                 4\n",
       "#forabol卐onarogenocida    4\n",
       "#bolsonaro2022            4\n",
       "#12setforabolsonaro       3\n",
       "#vacina                   3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(frequent_top_10_hashtags['hashtag_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190730df-c95c-4e06-9685-1bb29ecf202f",
   "metadata": {},
   "source": [
    "#### Most frequent hashtags throughout the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f1f3789-76f1-46f1-92d8-cfb96d028f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []\n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    for file in file_lists[week]['query_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for hashtag_list in df['hashtags']:\n",
    "            hashtag_list = eval(hashtag_list)\n",
    "            if hashtag_list:\n",
    "                for hashtag in hashtag_list:\n",
    "                    hashtags['query_ext'].append(hashtag.lower())\n",
    "    for file in file_lists[week]['hashtag_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for hashtag_list in df['hashtags']:\n",
    "            hashtag_list = eval(hashtag_list)\n",
    "            if hashtag_list:\n",
    "                for hashtag in hashtag_list:\n",
    "                    hashtags['hashtag_ext'].append(hashtag.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2357045a-b0c2-4352-ae3f-a143d35d2def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpidacovid        9839\n",
       "cpidapandemia     4109\n",
       "cpi               2928\n",
       "forabolsonaro     2047\n",
       "cpidocirco        1460\n",
       "globonews         1350\n",
       "covid             1113\n",
       "cpidogenocidio    1084\n",
       "pandemia          1078\n",
       "covid19           1021\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hashtags['query_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9581585-522c-4914-86af-1904dfb2d1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpidacovid               597552\n",
       "forabolsonaro            580075\n",
       "forabolsonarogenocida    147935\n",
       "cpidapandemia            140279\n",
       "renanvagabundo           115554\n",
       "cpidocirco               107150\n",
       "renansabiadetudo          88848\n",
       "29mforabolsonaro          79660\n",
       "euautorizopresidente      79111\n",
       "cpidotse                  56165\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hashtags['hashtag_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83baa7ab-9932-4621-9cb0-d799ea9de7ff",
   "metadata": {},
   "source": [
    "### __Topic Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934ade0c-d05f-4b53-8f77-07126dcce15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e86450-a7d6-4e58-8358-7406af530af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []\n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    for file in file_lists[week]['query_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for tweet in df['content']:\n",
    "            docs['query_ext'].append(tweet)\n",
    "    for file in file_lists[week]['hashtag_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for tweet in df['content']:\n",
    "            docs['hashtag_ext'].append(tweet)\n",
    "   \n",
    "# removing duplicated tweets\n",
    "for key in docs:\n",
    "    tweets_series = pd.Series(docs[key])\n",
    "    tweets_series.drop_duplicates(inplace=True)\n",
    "    docs[key] = tweets_series.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f90473-2584-4423-a6b4-d2234ca51c0e",
   "metadata": {},
   "source": [
    "#### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf2bd7b-cdc6-4106-a40c-4d60623a6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "stop_words.append('pra')\n",
    "stop_words.append('tá')\n",
    "stop_words.append('sobre')\n",
    "\n",
    "def remove_emoji(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "def remove_link(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)\n",
    "    tweet = tweet.strip('[link]')\n",
    "    tweet = re.sub(r'pic.twitter\\S+', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_av(tweet):\n",
    "    tweet = re.sub(r'VIDEO:', '', tweet)\n",
    "    tweet = re.sub(r'AUDIO:', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def tweet_preproc(tweet):\n",
    "    '''\n",
    "    Remove: @mentions, #hashtags, URL\n",
    "    links, punctuation and emojis,\n",
    "    and multiple white spaces\n",
    "    '''\n",
    "    tweet = remove_emoji(tweet)\n",
    "    tweet = remove_link(tweet)\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet = re.sub(r'\\d', '', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = [word for word in tweet.split() if len(word)>2 and word not in stop_words]\n",
    "    \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14e732ec-3ab1-4791-9678-d9bedaa2b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in docs:\n",
    "    doc = []\n",
    "    for tweet in docs[key]:\n",
    "        doc.append(tweet_preproc(tweet))\n",
    "    docs[key] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e29c4e-34e7-4d6a-a315-bb41b472db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amount of tweets\n",
      "- Query dataset: 384798\n",
      "- Hashtag dataset: 2917027\n",
      "\n"
     ]
    }
   ],
   "source": [
    " print(f'''\n",
    "Amount of tweets\n",
    "- Query dataset: {len(docs['query_ext'])}\n",
    "- Hashtag dataset: {len(docs['hashtag_ext'])}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63d7f093-db98-4920-9209-c645bda516e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpi',\n",
       " 'covid',\n",
       " 'faz',\n",
       " 'rir',\n",
       " 'jovem',\n",
       " 'bando',\n",
       " 'corrupto',\n",
       " 'vai',\n",
       " 'fazer',\n",
       " 'alguma',\n",
       " 'coisa',\n",
       " 'além',\n",
       " 'proselitismo',\n",
       " 'político',\n",
       " 'estude',\n",
       " 'definição',\n",
       " 'genocídio',\n",
       " 'pare',\n",
       " 'passar',\n",
       " 'vergonha',\n",
       " 'cairão',\n",
       " 'lado',\n",
       " 'mil',\n",
       " 'direita',\n",
       " 'serás',\n",
       " 'atingindo']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs['query_ext'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f922554a-5d89-4154-ba38-e86d9fbb3231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fique', 'certo', 'extrema', 'esquerda', 'vai', 'ser', 'contra']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs['hashtag_ext'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37abed-a55c-4971-9906-e01bcdecd465",
   "metadata": {},
   "source": [
    "#### Train STTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b99489e9-0e79-4513-8a0b-44b9799610ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm import MovieGroupProcess # loading gsdmm model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183cd15-5d84-402c-ac3d-3df33297fa9e",
   "metadata": {},
   "source": [
    "### __QUERY DATASET__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "203076f4-ba0f-4e01-a6ba-7719ad859219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY DATASET\n",
      "Vocabulary size: 126808\n",
      "Number of documents: 384798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = set(x for doc in docs['query_ext'] for x in doc)\n",
    "n_terms = len(vocab)\n",
    "\n",
    "print(f'''\n",
    "QUERY DATASET\n",
    "Vocabulary size: {n_terms}\n",
    "Number of documents: {len(docs['query_ext'])}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfd2c60c-89ae-46b6-8864-0b7eb285c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 344850 clusters with 15 clusters populated\n",
      "In stage 1: transferred 251339 clusters with 15 clusters populated\n",
      "In stage 2: transferred 157361 clusters with 15 clusters populated\n",
      "In stage 3: transferred 108726 clusters with 15 clusters populated\n",
      "In stage 4: transferred 88771 clusters with 15 clusters populated\n",
      "In stage 5: transferred 80332 clusters with 15 clusters populated\n",
      "In stage 6: transferred 75778 clusters with 15 clusters populated\n",
      "In stage 7: transferred 72383 clusters with 15 clusters populated\n",
      "In stage 8: transferred 69870 clusters with 15 clusters populated\n",
      "In stage 9: transferred 67981 clusters with 15 clusters populated\n",
      "In stage 10: transferred 66474 clusters with 15 clusters populated\n",
      "In stage 11: transferred 65361 clusters with 15 clusters populated\n",
      "In stage 12: transferred 64853 clusters with 15 clusters populated\n",
      "In stage 13: transferred 64585 clusters with 15 clusters populated\n",
      "In stage 14: transferred 64327 clusters with 15 clusters populated\n",
      "In stage 15: transferred 63669 clusters with 15 clusters populated\n",
      "In stage 16: transferred 63457 clusters with 15 clusters populated\n",
      "In stage 17: transferred 63318 clusters with 15 clusters populated\n",
      "In stage 18: transferred 63128 clusters with 15 clusters populated\n",
      "In stage 19: transferred 63091 clusters with 15 clusters populated\n",
      "In stage 20: transferred 62766 clusters with 15 clusters populated\n",
      "In stage 21: transferred 63059 clusters with 15 clusters populated\n",
      "In stage 22: transferred 62570 clusters with 15 clusters populated\n",
      "In stage 23: transferred 62699 clusters with 15 clusters populated\n",
      "In stage 24: transferred 62557 clusters with 15 clusters populated\n",
      "In stage 25: transferred 62457 clusters with 15 clusters populated\n",
      "In stage 26: transferred 62661 clusters with 15 clusters populated\n",
      "In stage 27: transferred 62554 clusters with 15 clusters populated\n",
      "In stage 28: transferred 62246 clusters with 15 clusters populated\n",
      "In stage 29: transferred 62533 clusters with 15 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp = MovieGroupProcess(K=15, alpha=0.10, beta=0.10, n_iters=30)\n",
    "\n",
    "y = mgp.fit(docs['query_ext'], n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d1b0793-20e7-4b35-918b-7f11fb1615af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5884314e-a850-48d7-99fb-f739b05af2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic:  [26524 32335 51602 21594 52740 17342  3359 25628 16068 32812 35296 10032\n",
      "  7966 24321 27179]\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents per topic: ', doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03a7780b-fd15-4797-964a-6d685ff65bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important clusters (by number of docs inside): [ 4  2 10  9  1 14  0  7 13  3  5  8 11 12  6]\n"
     ]
    }
   ],
   "source": [
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42f7a913-08b4-4b7b-97ea-fda02417c3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 4 : [('cpi', 55874), ('covid', 36695), ('pandemia', 19450), ('bolsonaro', 10136), ('governo', 8926), ('vai', 7463), ('brasil', 5631), ('ser', 5139), ('presidente', 4753), ('mortes', 3806), ('mil', 3661), ('nada', 3598), ('contra', 3507), ('tudo', 3464), ('agora', 3259)]\n",
      "\n",
      "Cluster 2 : [('cpi', 52794), ('covid', 45385), ('pandemia', 6582), ('hoje', 4249), ('vai', 3974), ('dia', 3698), ('assistir', 3268), ('ver', 3129), ('bbb', 2874), ('gente', 2862), ('melhor', 2714), ('assistindo', 2367), ('entretenimento', 2114), ('vendo', 1865), ('nada', 1755)]\n",
      "\n",
      "Cluster 10 : [('cpi', 37495), ('covid', 28087), ('renan', 9889), ('pandemia', 8080), ('bolsonaro', 8076), ('calheiros', 6734), ('presidente', 6482), ('senador', 5862), ('aziz', 4274), ('via', 3983), ('relator', 3960), ('omar', 3886), ('governadores', 3345), ('convocação', 3301), ('senadores', 3113)]\n",
      "\n",
      "Cluster 9 : [('cpi', 34824), ('covid', 27610), ('pandemia', 5977), ('senador', 4784), ('bolsonaro', 3994), ('vai', 3616), ('ser', 2439), ('cara', 2165), ('presidente', 2033), ('hoje', 1827), ('agora', 1765), ('brasil', 1716), ('renan', 1537), ('cloroquina', 1445), ('falar', 1432)]\n",
      "\n",
      "Cluster 1 : [('cpi', 36278), ('covid', 23558), ('pandemia', 8875), ('presidente', 5248), ('renan', 4496), ('circo', 3694), ('stf', 3520), ('bolsonaro', 3349), ('brasil', 3108), ('povo', 2859), ('calheiros', 2856), ('ser', 2773), ('senadores', 2746), ('vai', 2727), ('senado', 2350)]\n",
      "\n",
      "Cluster 14 : [('cpi', 28536), ('covid', 21437), ('pandemia', 6295), ('saúde', 3892), ('covaxin', 3651), ('ouve', 3541), ('depoimento', 3288), ('precisa', 3265), ('ministério', 3006), ('ricardo', 3001), ('bolsonaro', 2918), ('barros', 2783), ('sigilo', 2643), ('miranda', 2543), ('governo', 2388)]\n",
      "\n",
      "Cluster 0 : [('cpi', 27799), ('covid', 20533), ('pandemia', 6910), ('senador', 4940), ('senadores', 3110), ('tratamento', 2387), ('nise', 2211), ('dra', 2038), ('hoje', 2008), ('parabéns', 2004), ('ser', 1987), ('médicos', 1833), ('médica', 1799), ('contra', 1752), ('cloroquina', 1640)]\n",
      "\n",
      "Cluster 7 : [('cpi', 26560), ('covid', 19644), ('pandemia', 7279), ('saúde', 6274), ('depoimento', 5613), ('pazuello', 5247), ('queiroga', 3587), ('vivo', 3357), ('exministro', 3172), ('ouve', 2929), ('araújo', 2914), ('via', 2869), ('ministro', 2833), ('nesta', 2369), ('bolsonaro', 2266)]\n",
      "\n",
      "Cluster 13 : [('cpi', 27092), ('covid', 12961), ('pandemia', 12795), ('governadores', 7433), ('dinheiro', 6170), ('prefeitos', 4354), ('investigar', 4176), ('federal', 3715), ('governo', 3695), ('presidente', 3496), ('combate', 3140), ('saúde', 3025), ('estados', 2757), ('desvios', 2569), ('ser', 2453)]\n",
      "\n",
      "Cluster 3 : [('cpi', 22563), ('covid', 16101), ('pandemia', 7611), ('bolsonaro', 3988), ('governo', 3647), ('senado', 1674), ('brasil', 1561), ('presidente', 1412), ('comissão', 1392), ('contra', 1354), ('hoje', 1303), ('via', 1265), ('semana', 1250), ('dia', 1182), ('federal', 1134)]\n",
      "\n",
      "Cluster 5 : [('cpi', 17593), ('covid', 16534), ('cloroquina', 4481), ('prevent', 3993), ('bolsonaro', 3505), ('senior', 3079), ('pandemia', 2867), ('diz', 2593), ('tratamento', 2497), ('contra', 2468), ('saúde', 1940), ('médicos', 1656), ('kit', 1644), ('anvisa', 1590), ('presidente', 1585)]\n",
      "\n",
      "Cluster 8 : [('cpi', 16845), ('covid', 14308), ('pazuello', 7003), ('stf', 2590), ('depor', 2526), ('pandemia', 2259), ('depoimento', 2215), ('vai', 2167), ('silêncio', 2144), ('ficar', 1947), ('direito', 1389), ('corpus', 1358), ('ser', 1340), ('habeas', 1324), ('carlos', 1289)]\n",
      "\n",
      "Cluster 11 : [('cpi', 10271), ('covid', 8323), ('governo', 2947), ('bolsonaro', 2616), ('pfizer', 2529), ('pandemia', 2329), ('vacinas', 2266), ('vacina', 1887), ('brasil', 1523), ('milhões', 1198), ('presidente', 1114), ('doses', 1080), ('diz', 1036), ('saúde', 1009), ('compra', 862)]\n",
      "\n",
      "Cluster 12 : [('cpi', 8189), ('covid', 6293), ('bolsonaro', 2186), ('pandemia', 1843), ('via', 1610), ('senador', 889), ('governo', 795), ('senado', 775), ('senadores', 664), ('diz', 640), ('planalto', 602), ('nacional', 568), ('presidente', 528), ('ciro', 521), ('assine', 502)]\n",
      "\n",
      "Cluster 6 : [('cpi', 5069), ('covid', 3944), ('link', 1872), ('anexo', 1438), ('pandemia', 986), ('democracia', 714), ('vai', 702), ('pizza', 646), ('poder', 636), ('brasil', 618), ('min', 495), ('notícia', 487), ('ltlt', 479), ('informacional', 477), ('considere', 463)]\n"
     ]
    }
   ],
   "source": [
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(mgp.cluster_word_distribution, top_index, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f13e6-dbce-42f1-8069-5b7456dc5e2b",
   "metadata": {},
   "source": [
    "### __HASHTAG DATASET__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d328494-3d29-4011-9ce4-b815f2fa9dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HASHTAG DATASET\n",
      "Vocabulary size: 388370\n",
      "Number of documents: 2917027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = set(x for doc in docs['hashtag_ext'] for x in doc)\n",
    "n_terms = len(vocab)\n",
    "\n",
    "print(f'''\n",
    "HASHTAG DATASET\n",
    "Vocabulary size: {n_terms}\n",
    "Number of documents: {len(docs['hashtag_ext'])}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0d38b00-580e-4f06-b8ee-05f73ae85829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 2675951 clusters with 15 clusters populated\n",
      "In stage 1: transferred 2400120 clusters with 15 clusters populated\n",
      "In stage 2: transferred 1827759 clusters with 15 clusters populated\n",
      "In stage 3: transferred 1399156 clusters with 15 clusters populated\n",
      "In stage 4: transferred 1214683 clusters with 15 clusters populated\n",
      "In stage 5: transferred 1132556 clusters with 15 clusters populated\n",
      "In stage 6: transferred 1090232 clusters with 15 clusters populated\n",
      "In stage 7: transferred 1065423 clusters with 15 clusters populated\n",
      "In stage 8: transferred 1048628 clusters with 15 clusters populated\n",
      "In stage 9: transferred 1038027 clusters with 15 clusters populated\n",
      "In stage 10: transferred 1028141 clusters with 15 clusters populated\n",
      "In stage 11: transferred 1019920 clusters with 15 clusters populated\n",
      "In stage 12: transferred 1014338 clusters with 15 clusters populated\n",
      "In stage 13: transferred 1010501 clusters with 15 clusters populated\n",
      "In stage 14: transferred 1007743 clusters with 15 clusters populated\n",
      "In stage 15: transferred 1005338 clusters with 15 clusters populated\n",
      "In stage 16: transferred 1002969 clusters with 15 clusters populated\n",
      "In stage 17: transferred 1000450 clusters with 15 clusters populated\n",
      "In stage 18: transferred 996553 clusters with 15 clusters populated\n",
      "In stage 19: transferred 995109 clusters with 15 clusters populated\n",
      "In stage 20: transferred 993103 clusters with 15 clusters populated\n",
      "In stage 21: transferred 992727 clusters with 15 clusters populated\n",
      "In stage 22: transferred 990826 clusters with 15 clusters populated\n",
      "In stage 23: transferred 988400 clusters with 15 clusters populated\n",
      "In stage 24: transferred 985721 clusters with 15 clusters populated\n",
      "In stage 25: transferred 983326 clusters with 15 clusters populated\n",
      "In stage 26: transferred 981015 clusters with 15 clusters populated\n",
      "In stage 27: transferred 980223 clusters with 15 clusters populated\n",
      "In stage 28: transferred 979256 clusters with 15 clusters populated\n",
      "In stage 29: transferred 979093 clusters with 15 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp = MovieGroupProcess(K=15, alpha=0.10, beta=0.10, n_iters=30)\n",
    "\n",
    "y = mgp.fit(docs['hashtag_ext'], n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b80b18b3-7e52-43d0-ba2a-40da71776435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58b7c41a-9dfc-4e46-b704-2fba9cec17bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic:  [160053 352787 213510 375960 133923 208441 183125 183696 319631 160444\n",
      "  56922 152949 142283  73004 200299]\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents per topic: ', doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8459b5f-7e5c-4b95-b6c9-954404f4ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important clusters (by number of docs inside): [ 3  1  8  2  5 14  7  6  9  0 11 12  4 13 10]\n"
     ]
    }
   ],
   "source": [
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96c6ed83-8bd2-43b4-ba70-564f7ca467ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 3 : [('senador', 34821), ('cpi', 29240), ('marcos', 25114), ('vai', 24973), ('rogério', 23628), ('cara', 21547), ('hoje', 20633), ('omar', 14361), ('falar', 13660), ('gente', 13547), ('agora', 13323), ('fala', 12793), ('ser', 11389), ('heinze', 10875), ('falando', 10741)]\n",
      "\n",
      "Cluster 1 : [('bolsonaro', 30222), ('ser', 19846), ('gente', 19379), ('presidente', 17226), ('brasil', 16905), ('vai', 16206), ('ainda', 13918), ('povo', 13889), ('hoje', 12588), ('governo', 11955), ('esquerda', 11891), ('nada', 11829), ('dia', 11651), ('tudo', 11541), ('agora', 11495)]\n",
      "\n",
      "Cluster 8 : [('vai', 58163), ('bolsonaro', 34441), ('agora', 14656), ('ser', 14394), ('dia', 14140), ('hoje', 11927), ('gente', 10446), ('brasil', 9595), ('ver', 9545), ('aqui', 8650), ('cair', 8436), ('vou', 8435), ('casa', 8359), ('presidente', 8309), ('ter', 8145)]\n",
      "\n",
      "Cluster 2 : [('jforabolsonaro', 62438), ('mforabolsonaro', 37352), ('bolsonaro', 31648), ('dia', 22588), ('jpovonasruas', 20694), ('ruas', 19279), ('hoje', 18080), ('mpovonasruas', 16299), ('povo', 16152), ('brasil', 15728), ('genocida', 15203), ('contra', 13790), ('ato', 11224), ('governo', 9911), ('vamos', 9872)]\n",
      "\n",
      "Cluster 5 : [('cpi', 42505), ('vergonha', 16253), ('circo', 13781), ('povo', 13587), ('senadores', 11943), ('presidente', 11202), ('ser', 10936), ('vai', 10824), ('brasil', 10730), ('senador', 10635), ('todos', 9726), ('dinheiro', 9534), ('gabas', 8879), ('governo', 8250), ('governadores', 7906)]\n",
      "\n",
      "Cluster 14 : [('brasil', 42941), ('presidente', 25105), ('povo', 23605), ('bolsonaro', 18612), ('deus', 16881), ('todos', 13065), ('dia', 12363), ('país', 10997), ('vamos', 10835), ('ser', 9348), ('tudo', 8686), ('melhor', 8230), ('parabéns', 8110), ('brasileiro', 7889), ('bem', 7178)]\n",
      "\n",
      "Cluster 7 : [('vagabundo', 20513), ('bolsonaro', 16420), ('ladrão', 15703), ('renan', 12174), ('genocida', 12090), ('corrupto', 10690), ('ser', 10047), ('pode', 7440), ('presidente', 7376), ('lula', 7348), ('cadeia', 7032), ('lixo', 6178), ('vai', 6046), ('brasil', 5621), ('canalha', 5571)]\n",
      "\n",
      "Cluster 6 : [('dia', 33607), ('bom', 29580), ('vamos', 16702), ('bora', 14915), ('boa', 13032), ('tag', 9441), ('jforabolsonaro', 8692), ('hoje', 7569), ('bolsonaro', 7409), ('todos', 7155), ('noite', 6121), ('mforabolsonaro', 5895), ('sdv', 5584), ('seguindo', 5376), ('tarde', 5163)]\n",
      "\n",
      "Cluster 9 : [('voto', 31560), ('povo', 16460), ('bolsonaro', 16268), ('presidente', 14797), ('impresso', 13640), ('contra', 10820), ('stf', 10653), ('auditável', 10279), ('ser', 9541), ('vai', 9404), ('eleições', 8483), ('impeachment', 7807), ('urnas', 7802), ('tse', 7654), ('brasil', 7565)]\n",
      "\n",
      "Cluster 0 : [('cpi', 23597), ('bolsonaro', 19545), ('governo', 14342), ('presidente', 11481), ('saúde', 10848), ('pazuello', 10124), ('senador', 9005), ('vacina', 8868), ('vai', 8657), ('ser', 8346), ('depoimento', 7959), ('diz', 7936), ('miranda', 7852), ('ministério', 7459), ('ricardo', 7353)]\n",
      "\n",
      "Cluster 11 : [('bolsonaro', 23242), ('governo', 19466), ('brasil', 16002), ('povo', 13404), ('país', 10594), ('mil', 9823), ('corrupção', 9677), ('contra', 9043), ('dinheiro', 8749), ('genocida', 8503), ('jforabolsonaro', 8261), ('vai', 8040), ('público', 7963), ('fome', 7025), ('brasileiros', 6891)]\n",
      "\n",
      "Cluster 12 : [('vacina', 20498), ('mil', 11521), ('dose', 10573), ('hoje', 9422), ('dia', 9286), ('bolsonaro', 9075), ('mforabolsonaro', 8779), ('ter', 8160), ('viva', 8159), ('covid', 7705), ('pessoas', 7565), ('genocida', 7265), ('vidas', 7155), ('todos', 6453), ('sus', 6398)]\n",
      "\n",
      "Cluster 4 : [('cloroquina', 16520), ('covid', 13158), ('tratamento', 11635), ('cpi', 9825), ('senador', 9500), ('saúde', 9064), ('ser', 8539), ('precoce', 8165), ('bolsonaro', 8064), ('governo', 7904), ('pandemia', 6702), ('brasil', 6030), ('presidente', 6027), ('contra', 5987), ('nise', 5926)]\n",
      "\n",
      "Cluster 13 : [('covid', 12324), ('brasil', 10910), ('casos', 6254), ('contra', 6074), ('milhões', 5901), ('mortes', 5670), ('vacina', 5357), ('vacinas', 5222), ('doses', 5198), ('pandemia', 4825), ('governo', 4632), ('vacinação', 4581), ('mil', 4387), ('saúde', 4317), ('bolsonaro', 4288)]\n",
      "\n",
      "Cluster 10 : [('rede', 68492), ('núcleo', 47866), ('luciano', 47808), ('lepera', 47034), ('direitos', 21917), ('curta', 21450), ('inscrevase', 14143), ('jforabolsonaro', 12882), ('réuconfesso', 12257), ('bolsonaro', 9874), ('sforabolsonaro', 7548), ('fortaleça', 7164), ('outforabolsonaro', 5953), ('tácaroculpadobolsonaro', 5677), ('diz', 4015)]\n"
     ]
    }
   ],
   "source": [
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(mgp.cluster_word_distribution, top_index, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
