{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c467f6bd-a30f-4509-9dcb-bfd42c3bebec",
   "metadata": {},
   "source": [
    "# __Extracted Tweets Data Analyis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d79ccc-bcbc-4784-aab6-944e1439fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, json, datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4c0ea9-2f40-4c8f-b33d-a6b7054ab5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of extracted weeks:  21\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/tweets/'\n",
    "\n",
    "week_list = [week_dir for week_dir in os.listdir(DATA_PATH) if os.path.isdir(DATA_PATH+week_dir) and not week_dir.endswith('.ipynb_checkpoints')]\n",
    "print('Amount of extracted weeks: ', len(week_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ff40da-03e3-4dcd-9478-c10c7555dd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['week_01',\n",
       " 'week_02',\n",
       " 'week_03',\n",
       " 'week_04',\n",
       " 'week_05',\n",
       " 'week_06',\n",
       " 'week_07',\n",
       " 'week_08',\n",
       " 'week_09',\n",
       " 'week_10',\n",
       " 'week_11',\n",
       " 'week_12',\n",
       " 'week_pr_01',\n",
       " 'week_pr_02',\n",
       " 'week_13',\n",
       " 'week_14',\n",
       " 'week_15',\n",
       " 'week_16',\n",
       " 'week_17',\n",
       " 'week_pr_03',\n",
       " 'week_18']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_list.sort()\n",
    "week_list.remove('week_pr_01')\n",
    "week_list.insert(week_list.index('week_12')+1, 'week_pr_01')\n",
    "week_list.remove('week_pr_02')\n",
    "week_list.insert(week_list.index('week_pr_01')+1, 'week_pr_02')\n",
    "week_list.remove('week_pr_03')\n",
    "week_list.insert(week_list.index('week_17')+1, 'week_pr_03')\n",
    "week_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4eee93-7e14-4442-a23a-6de8a03f6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "tts_amount = {\n",
    "    'week': {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    },\n",
    "    'day': {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    }\n",
    "}\n",
    "frequent_top_10_hashtags = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []\n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    for file in os.listdir(DATA_PATH+week):\n",
    "        if file.endswith('.json'):\n",
    "            with open(f'{DATA_PATH+week}/{file}', encoding='utf-8') as week_info:\n",
    "                data = json.load(week_info)\n",
    "                tts_amount['week']['query_ext'].append(data['tweets_amount']['query_ext'])\n",
    "                tts_amount['week']['hashtag_ext'].append(data['tweets_amount']['hashtag_ext'])\n",
    "                for hashtag in data['top_10_hashtags']['query_ext']:\n",
    "                    frequent_top_10_hashtags['query_ext'].append(hashtag)\n",
    "                for hashtag in data['top_10_hashtags']['hashtag_ext']:\n",
    "                    frequent_top_10_hashtags['hashtag_ext'].append(hashtag)\n",
    "                for day in data['days_info']:\n",
    "                    days.append(f'{week}_{day}')\n",
    "                    tts_amount['day']['query_ext'].append(data['days_info'][day]['tweets_amount']['query_ext'])\n",
    "                    tts_amount['day']['hashtag_ext'].append(data['days_info'][day]['tweets_amount']['hashtag_ext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c37d0-935c-4782-a1ab-a7b6dd15c173",
   "metadata": {},
   "source": [
    "## __Quantitative Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b477f933-53c5-441a-a3c3-d9266a03ca51",
   "metadata": {},
   "source": [
    "### __Tweets Amount__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e15ca7-3e2a-49f0-b521-523faaa88780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3842d9a84c9e412e94f8f8f6d8f113bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weekly_tts_amount = pd.DataFrame(tts_amount['week'])\n",
    "weekly_tts_amount['week'] = week_list\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(x='week', y='value', hue='dataset', data=pd.melt(weekly_tts_amount, ['week']).rename(columns={'variable':'dataset'}))\n",
    "ax.set(xlabel='week number', ylabel='amount of tweets')\n",
    "plt.title('Tweets amount per week')\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c146feaf-4c02-45f7-82de-049e129b72b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ffc1e1ecc24a6697676e3039039281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "daily_tts_amount = pd.DataFrame(tts_amount['day'])\n",
    "daily_tts_amount['day'] = days\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(x='day', y='value', hue='dataset', data=pd.melt(daily_tts_amount, ['day']).rename(columns={'variable':'dataset'}))\n",
    "ax.set(xlabel='days', ylabel='amount of tweets')\n",
    "plt.title('Tweets amount per day')\n",
    "plt.tick_params(\n",
    "    axis='x',\n",
    "    which='both',\n",
    "    bottom=False,\n",
    "    top=False,\n",
    "    labelbottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd22f3eb-bdc3-484b-996f-1d61a28aee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of weeks: 21\n",
      "\n",
      "Total number of tweets:\n",
      "- Query dataset: 350135\n",
      "- Hashtag dataset: 2736534\n",
      "\n",
      "Weeks with the highest amount of tweets:\n",
      "- Query dataset: \n",
      "[['week_02', 37097], ['week_04', 37037], ['week_03', 36058], ['week_01', 29618], ['week_05', 26749]]\n",
      "- Hashtag dataset: \n",
      "[['week_10', 255016], ['week_09', 246175], ['week_05', 230436], ['week_13', 197249], ['week_11', 192390]]\n",
      "\n",
      "Days with the highest amount of tweets:\n",
      "- Query dataset:\n",
      "[['week_03_day_4', 14333], ['week_02_day_3', 11035], ['week_01_day_3', 10495], ['week_04_day_4', 10262], ['week_04_day_3', 9957]]\n",
      "- Hashtag dataset:\n",
      "[['week_05_day_7', 131549], ['week_10_day_2', 87000], ['week_03_day_5', 83904], ['week_pr_03_day_3', 82177], ['week_08_day_7', 60863]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Total number of weeks: {len(week_list)}\n",
    "\n",
    "Total number of tweets:\n",
    "- Query dataset: {sum(tts_amount['week']['query_ext'])}\n",
    "- Hashtag dataset: {sum(tts_amount['week']['hashtag_ext'])}\n",
    "\n",
    "Weeks with the highest amount of tweets:\n",
    "- Query dataset: \n",
    "{weekly_tts_amount.sort_values(by='query_ext', ascending=False)[:5][['week','query_ext']].values.tolist()}\n",
    "- Hashtag dataset: \n",
    "{weekly_tts_amount.sort_values(by='hashtag_ext', ascending=False)[:5][['week','hashtag_ext']].values.tolist()}\n",
    "\n",
    "Days with the highest amount of tweets:\n",
    "- Query dataset:\n",
    "{daily_tts_amount.sort_values(by='query_ext', ascending=False)[:5][['day', 'query_ext']].values.tolist()}\n",
    "- Hashtag dataset:\n",
    "{daily_tts_amount.sort_values(by='hashtag_ext', ascending=False)[:5][['day', 'hashtag_ext']].values.tolist()}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741374cb-1356-4da7-b79b-04520ab848df",
   "metadata": {},
   "source": [
    "### __User Amount__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92eae57-59c3-4673-93ea-7f4fdf3d90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lists = {}\n",
    "user_count = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []    \n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    query_users = []\n",
    "    hashtag_users = []\n",
    "    file_lists[week] = {\n",
    "        'query_ext': [],\n",
    "        'hashtag_ext': []\n",
    "    }\n",
    "    for file in os.listdir(DATA_PATH+week):\n",
    "        if file.endswith('query_ext.parquet'):\n",
    "            file_lists[week]['query_ext'].append(file)\n",
    "        if file.endswith('hashtags_ext.parquet'):\n",
    "            file_lists[week]['hashtag_ext'].append(file)\n",
    "    file_lists[week]['query_ext'].sort()\n",
    "    file_lists[week]['hashtag_ext'].sort()\n",
    "    \n",
    "    for file in file_lists[week]['query_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for user in df['user']:\n",
    "            query_users.append(eval(user)['username'])\n",
    "    user_count['query_ext'].append(len(set(query_users)))\n",
    "    for file in file_lists[week]['hashtag_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for user in df['user']:\n",
    "            hashtag_users.append(eval(user)['username'])\n",
    "    user_count['hashtag_ext'].append(len(set(hashtag_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e537d4-aeca-46ae-bf81-a67403bdf260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1928205aae415ebe4d87f71ca30361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_count_df = pd.DataFrame(user_count)\n",
    "user_count_df['week'] = week_list\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(x='week', y='value', hue='dataset', data=pd.melt(user_count_df, ['week']).rename(columns={'variable':'dataset'}))\n",
    "ax.set(xlabel='week number', ylabel='amount of users')\n",
    "plt.title('Users amount per week')\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c08ee7-600c-4d89-8ded-00eb4177bbcc",
   "metadata": {},
   "source": [
    "## __Qualitative Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859f8ae-25c5-441f-a69a-398985175686",
   "metadata": {},
   "source": [
    "### __Hashtag Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95bcac-02ff-47b2-bb7a-476344fdbe75",
   "metadata": {},
   "source": [
    "#### Most frequent hashtags in the Top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4c01d2e-3415-467e-a0e8-825535310d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#forabolsonaro            17\n",
       "#cpi                      16\n",
       "#cpidapandemia            15\n",
       "#cpidacovid               13\n",
       "#forabolsonarogenocida    13\n",
       "#pandemia                 12\n",
       "#covid                    10\n",
       "#brasil                   10\n",
       "#covid19                   9\n",
       "#cpidocirco                9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(frequent_top_10_hashtags['query_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3078156e-f034-49f8-adcc-254e365332f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#cpidapandemia            4\n",
       "#forabol卐onarogenocida    4\n",
       "#cpidacovid19             4\n",
       "#fakenews                 3\n",
       "#forabolsonaro            3\n",
       "#coronavac                3\n",
       "#elenao                   3\n",
       "#bolsonaronacadeia        3\n",
       "#12setforabolsonaro       3\n",
       "#impeachmentja            3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(frequent_top_10_hashtags['hashtag_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190730df-c95c-4e06-9685-1bb29ecf202f",
   "metadata": {},
   "source": [
    "#### Most frequent hashtags throughout the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f1f3789-76f1-46f1-92d8-cfb96d028f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []\n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    for file in file_lists[week]['query_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for hashtag_list in df['hashtags']:\n",
    "            hashtag_list = eval(hashtag_list)\n",
    "            if hashtag_list:\n",
    "                for hashtag in hashtag_list:\n",
    "                    hashtags['query_ext'].append(hashtag.lower())\n",
    "    for file in file_lists[week]['hashtag_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for hashtag_list in df['hashtags']:\n",
    "            hashtag_list = eval(hashtag_list)\n",
    "            if hashtag_list:\n",
    "                for hashtag in hashtag_list:\n",
    "                    hashtags['hashtag_ext'].append(hashtag.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2357045a-b0c2-4352-ae3f-a143d35d2def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpidacovid        8984\n",
       "cpidapandemia     3763\n",
       "cpi               2616\n",
       "forabolsonaro     1908\n",
       "globonews         1176\n",
       "cpidocirco        1140\n",
       "cpidogenocidio    1066\n",
       "covid             1032\n",
       "pandemia          1008\n",
       "covid19            941\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hashtags['query_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9581585-522c-4914-86af-1904dfb2d1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forabolsonaro            536287\n",
       "cpidacovid               529892\n",
       "forabolsonarogenocida    140463\n",
       "cpidapandemia            125809\n",
       "renanvagabundo           101326\n",
       "renansabiadetudo          88846\n",
       "cpidocirco                82118\n",
       "29mforabolsonaro          79660\n",
       "euautorizopresidente      79109\n",
       "cpidotse                  56164\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hashtags['hashtag_ext']).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83baa7ab-9932-4621-9cb0-d799ea9de7ff",
   "metadata": {},
   "source": [
    "### __Topic Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b058907-2c97-4313-9891-ac1b02624c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14e86450-a7d6-4e58-8358-7406af530af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {\n",
    "    'query_ext': [],\n",
    "    'hashtag_ext': []\n",
    "}\n",
    "\n",
    "for week in week_list:\n",
    "    for file in file_lists[week]['query_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for tweet in df['content']:\n",
    "            docs['query_ext'].append(tweet)\n",
    "    for file in file_lists[week]['hashtag_ext']:\n",
    "        df = pd.read_parquet(f'{DATA_PATH+week}/{file}')\n",
    "        for tweet in df['content']:\n",
    "            docs['hashtag_ext'].append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef754a3e-f05c-46d8-ba39-0b461f08b2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b1e5a8b61d494bb62559fb05520460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-30 12:45:22,028 - BERTopic - Transformed documents to Embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(language='multilingual', calculate_probabilities=True, verbose=True)\n",
    "\n",
    "topics_q, probs_q = topic_model.fit_transform(docs['query_ext'])\n",
    "topics_h, probs_h = topic_model.fit_transform(docs['hashtag_ext'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-twitter-cpi] *",
   "language": "python",
   "name": "conda-env-.conda-twitter-cpi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
